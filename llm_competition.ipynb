{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d58b3d-f7fe-439e-993f-8c161cc854f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T14:29:03.778791Z",
     "iopub.status.busy": "2025-10-08T14:29:03.778475Z",
     "iopub.status.idle": "2025-10-08T14:29:06.859694Z",
     "shell.execute_reply": "2025-10-08T14:29:06.858971Z",
     "shell.execute_reply.started": "2025-10-08T14:29:03.778768Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from click import prompt\n",
    "import pandas as pd\n",
    "import ace_lib as ace\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from google import genai  # Using Google AI studio\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "FEEDBACK_FILE = \"feedback_memory.json\"\n",
    "\n",
    "def load_feedback_memory():\n",
    "    if os.path.exists(FEEDBACK_FILE):\n",
    "        with open(FEEDBACK_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {\"mistakes\": [], \"corrections\": []}\n",
    "\n",
    "def save_feedback_memory(memory):\n",
    "    with open(FEEDBACK_FILE, \"w\") as f:\n",
    "        json.dump(memory, f, indent=4)\n",
    "\n",
    "def record_feedback(problem, fix):\n",
    "    memory = load_feedback_memory()\n",
    "    memory[\"mistakes\"].append(problem)\n",
    "    memory[\"corrections\"].append(fix)\n",
    "    save_feedback_memory(memory)\n",
    "\n",
    "def build_feedback_context():\n",
    "    memory = load_feedback_memory()\n",
    "    if not memory[\"mistakes\"]:\n",
    "        return \"\"\n",
    "    summary = \"\\n\".join([\n",
    "        f\"- Previously, you made this mistake: '{m}' → Corrected as: '{c}'\"\n",
    "        for m, c in zip(memory[\"mistakes\"], memory[\"corrections\"])\n",
    "    ])\n",
    "    return f\"\\nImportant: Avoid repeating previous mistakes.\\n{summary}\\n\"\n",
    "\n",
    "\n",
    "# Small helper to handle rate limits / transient errors when calling Brain API\n",
    "def get_with_retries(session, url, params=None, max_retries=5, backoff_base=1.0):\n",
    "    \"\"\"Perform session.get with retries on 429 and 5xx errors.\n",
    "\n",
    "    Args:\n",
    "        session: requests-like session with .get\n",
    "        url: URL to fetch\n",
    "        params: optional params dict\n",
    "        max_retries: number of attempts\n",
    "        backoff_base: base seconds for exponential backoff\n",
    "\n",
    "    Returns:\n",
    "        Response object\n",
    "\n",
    "    Raises:\n",
    "        requests.exceptions.RequestException or the last exception encountered\n",
    "    \"\"\"\n",
    "    last_exc = None\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if params is not None:\n",
    "                r = session.get(url, params=params)\n",
    "            else:\n",
    "                r = session.get(url)\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            wait = backoff_base * (2 ** attempt) + random.random()\n",
    "            print(f\"Request exception: {e}. Retrying in {wait:.1f}s (attempt {attempt+1}/{max_retries})\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "\n",
    "        # If rate limited or server error, retry with backoff\n",
    "        if r.status_code == 429 or (500 <= r.status_code < 600):\n",
    "            wait = backoff_base * (2 ** attempt) + random.random()\n",
    "            print(f\"HTTP {r.status_code} received from {url}. Retrying in {wait:.1f}s (attempt {attempt+1}/{max_retries})\")\n",
    "            time.sleep(wait)\n",
    "            last_exc = Exception(f\"HTTP {r.status_code}\")\n",
    "            continue\n",
    "\n",
    "        # Success\n",
    "        return r\n",
    "\n",
    "    # All retries failed\n",
    "    if last_exc:\n",
    "        raise last_exc\n",
    "    raise Exception(\"Failed to get response\")\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "_llm_instance = None\n",
    "\n",
    "\n",
    "# Initialize Gemini client (new SDK)\n",
    "client = genai.Client(api_key=\"YOUR_API_KEY_HERE\")\n",
    "\n",
    "async def call_llm(prompt):\n",
    "    \"\"\"\n",
    "    Async interface with Google Gemini API using google-genai SDK.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"→ Sending prompt to Gemini 2.5 Flash Lite...\")\n",
    "\n",
    "        # Run the sync model call in a separate thread to stay async-safe\n",
    "        response = await asyncio.to_thread(\n",
    "            client.models.generate_content,\n",
    "            model=\"gemini-2.5-flash-lite\",\n",
    "            contents=(f\"You are a Quantitative Finance expert. While avoiding same combination of operators, help users explain alphas clearly. {prompt}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Extract text response\n",
    "        if hasattr(response, \"text\"):\n",
    "            return response.text.strip()\n",
    "        elif hasattr(response, \"candidates\"):\n",
    "            return response.candidates[0].content.parts[0].text.strip()\n",
    "        else:\n",
    "            print(\"⚠️ No valid response content.\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Gemini: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate English Description for Alpha\n",
    "async def generate_alpha_description(alpha_id, brain_session):\n",
    "    try:\n",
    "        #1. Fetch Alpha Details (use get_with_retries to handle rate limiting)\n",
    "        url = f\"https://api.worldquantbrain.com/alphas/{alpha_id}\"\n",
    "        response = get_with_retries(brain_session, url)\n",
    "        response.raise_for_status()\n",
    "        details = response.json()\n",
    "        alpha_expression = details.get('regular', {}).get('code', '')\n",
    "        alpha_settings = details.get('settings', {})\n",
    "        print(\"\\n[Success] Alpha details and settings fetched.\")\n",
    "        \n",
    "\n",
    "        # If needed get operators or other data\n",
    "        operators = ace.get_operators(brain_session)\n",
    "        operator_id_column = 'name'\n",
    "        all_operators_names = set(operators[operator_id_column])\n",
    "\n",
    "        #Parse alpha expression to find used operators\n",
    "        print(\"\\nParsing alpha expression to indentify components...\")\n",
    "        tokens = set(re.findall(r'[a-zA-Z0-9_.]+', alpha_expression))\n",
    "        operators_used = sorted(list(tokens.intersection(all_operators_names)))\n",
    "        potential_data_fields = tokens - all_operators_names\n",
    "        data_fields_used = sorted([f for f in potential_data_fields if not f[0].isdigit() and len(f) > 4])\n",
    "        print(f\" > operators identified: {operators_used}\")\n",
    "        print(f\" > data fields identified: {data_fields_used}\")\n",
    "\n",
    "        # 4. Fetch context for indentified components\n",
    "        #Operators context\n",
    "        operators_context = \"[]\"\n",
    "        if operators_used:\n",
    "            operators_df = operators[operators[operator_id_column].isin(operators_used)]\n",
    "            operators_context = operators_df[[operator_id_column, 'description', 'definition']].to_json(orient='records')\n",
    "\n",
    "        #Data fields context\n",
    "        data_field_context_list = []\n",
    "        if data_fields_used:\n",
    "            print(f\"\\nFetching descriptions for data fields...\")\n",
    "            for field in data_fields_used:\n",
    "                print(f\" > searching for '{field}'...\")\n",
    "                # Use alpha's specific settings for an accurate search\n",
    "                field_df = ace.get_datafields(\n",
    "                    s=brain_session,\n",
    "                     search=field,\n",
    "                      region=alpha_settings.get('region', 'USA'),\n",
    "                      universe=alpha_settings.get('universe', 'TOP3000'),\n",
    "                       delay=alpha_settings.get('delay', 1),\n",
    "                       data_type= 'ALL' #Search across all data types\n",
    "                )\n",
    "                if not field_df.empty:\n",
    "                    #Find the exact match\n",
    "                    exact_match = field_df[field_df['id'] == field]\n",
    "                    if not exact_match.empty:\n",
    "                        data_field_context_list.append(exact_match)\n",
    "                    else:\n",
    "                        print(f\"    -Warning: Found potential match for '{field}', but no exact match.\")\n",
    "                else:\n",
    "                    print(f\"    -Warning: No datafield found for '{field}' with current alpha settings.\")\n",
    "        \n",
    "        data_field_context = \"[]\"\n",
    "        if data_field_context_list:\n",
    "            final_df = pd.concat(data_field_context_list, ignore_index=True)\n",
    "            data_field_context = final_df[['id', 'description']].to_json(orient='records')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating alpha description: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Generate English description using call_llm\n",
    "    \n",
    "    operators_json = operators[operators['scope'] == 'REGULAR'][['name', 'description', 'definition']].to_json(orient='records')\n",
    "    data_json = data_field_context if isinstance(data_field_context, str) else json.dumps(data_field_context)\n",
    "\n",
    "    # Now build the prompt\n",
    "    prompt = f\"\"\"\n",
    "    Describe the following alpha in plain English.\n",
    "    Alpha: {alpha_expression}\n",
    "    Here are the components in JSON:\n",
    "    \"operators\": {operators_json},\n",
    "    \"data_fields\": {data_json}\n",
    "    \"\"\"\n",
    "    description = await call_llm(prompt)\n",
    "    if description:\n",
    "        return description.strip()\n",
    "    else:\n",
    "        print(\"⚠️ LLM returned no response.\")\n",
    "        return \"Error: LLM returned no response.\"\n",
    "\n",
    "\n",
    "# Generate new Alphas based on generated description\n",
    "async def generate_new_alphas(alpha_description, brain_session): \n",
    "    num_alphas = 5                                   #Number of Alphas\n",
    "\n",
    "    # Fetch operator and data context\n",
    "    operators = ace.get_operators(brain_session)\n",
    "    dataset_ids = ['analyst10', 'analyst14']             # Choosing datasets\n",
    "    data_fields = pd.concat(\n",
    "        [\n",
    "            ace.get_datafields(\n",
    "                brain_session,\n",
    "                region='USA',\n",
    "                universe='TOP3000',\n",
    "                delay=1,\n",
    "                dataset_id=dataset_id,\n",
    "                data_type='ALL'\n",
    "            )\n",
    "            for dataset_id in dataset_ids\n",
    "        ],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Convert to JSON\n",
    "\n",
    "    def truncate_json_records(df, cols, max_records):\n",
    "        if len(df) > max_records:\n",
    "            print(f\"⚠️ Truncating {len(df)} records to {max_records} to reduce prompt size.\")\n",
    "            df = df.sample(max_records, random_state=42)\n",
    "        return df[cols].to_json(orient='records')\n",
    "\n",
    "    operators_json = truncate_json_records(\n",
    "        operators[operators['scope'] == 'REGULAR'], \n",
    "        ['name', 'description', 'definition'], \n",
    "        30\n",
    "    )\n",
    "\n",
    "    data_fields_json = truncate_json_records(\n",
    "        data_fields, \n",
    "        ['id', 'description', 'category', 'type'], \n",
    "        50\n",
    "    )\n",
    "\n",
    "    feedback_context = build_feedback_context()\n",
    "\n",
    "    # Build the prompt — concise and strict\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following description: '{alpha_description}', generate {num_alphas} new alpha expressions using the provided operators and data.\n",
    "    {feedback_context}\n",
    "    Operators: {operators[operators['scope']=='REGULAR'].to_json()}, data {data_fields.to_json()} where id is data field name\n",
    "    Important: You can use type=MATRIX field by itself, as input to Arithmetic, \n",
    "    Cross Sectional, Time Series operators, With Logical and Transformational operators, As group in Group operators, with bucket().\n",
    "    You can’t use type=VECTOR field by itself. You only can use type=VECTOR field as input to Vector operator. Then you can treat it as a MATRIX field.\n",
    "    Always wrap type=VECTOR data in category=Vector operator.\n",
    "    You can’t use type=GROUP field by itself. You need to use it as “group” parameter in Group operator.\n",
    "\n",
    "    Provide only {num_alphas} alpha expressions, they should not be the same.\n",
    "    \"\"\"\n",
    "\n",
    "    # Call the LLM\n",
    "    response = await call_llm(prompt)\n",
    "    if not response:\n",
    "        print(\"⚠️ LLM returned no response while generating new alphas.\")\n",
    "        return []\n",
    "\n",
    "    # Try to parse as JSON safely\n",
    "    new_alphas_json = []  # Ensure variable exists even if parsing fails\n",
    "    try:\n",
    "        clean_response = response.strip()\n",
    "        # Remove Markdown fences like ```json or ```\n",
    "        clean_response = re.sub(r\"^```(?:json)?|```$\", \"\", clean_response, flags=re.MULTILINE).strip()\n",
    "\n",
    "        # Parse JSON\n",
    "        new_alphas_json = json.loads(clean_response)\n",
    "        print(\"✅ Successfully parsed new alphas as JSON.\")\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"⚠️ Response is not valid JSON. Attempting fallback parsing.\")\n",
    "        clean_lines = [\n",
    "            line.strip()\n",
    "            for line in response.split(\"\\n\")\n",
    "            if line.strip() and not line.strip().startswith(\"```\")\n",
    "        ]\n",
    "        new_alphas_json = [\n",
    "            {\"alpha_name\": f\"Alpha_{i+1}\", \"expression\": line}\n",
    "            for i, line in enumerate(clean_lines)\n",
    "        ]\n",
    "\n",
    "    # Remove duplicates by operator signature and record feedback\n",
    "    if new_alphas_json:\n",
    "        def operator_signature(expr):\n",
    "            ops = re.findall(r\"[a-z_]+\", expr)\n",
    "            return \"_\".join(sorted(set(ops)))\n",
    "\n",
    "        unique_alphas = []\n",
    "        seen_sigs = set()\n",
    "\n",
    "        for alpha in new_alphas_json:\n",
    "            sig = operator_signature(alpha[\"expression\"])\n",
    "            if sig in seen_sigs:\n",
    "                record_feedback(\n",
    "                    problem=f\"Repeated operator combination: {sig}\",\n",
    "                    fix=\"Enforce stronger variation across economic themes and operator types.\"\n",
    "                )\n",
    "                continue\n",
    "            seen_sigs.add(sig)\n",
    "            unique_alphas.append(alpha)\n",
    "\n",
    "        new_alphas_json = unique_alphas\n",
    "    else:\n",
    "        print(\"⚠️ No valid alphas generated after parsing.\")\n",
    "\n",
    "    return new_alphas_json\n",
    "\n",
    "async def main():\n",
    "    # Start Brain session\n",
    "    brain_session = ace.start_session()\n",
    "\n",
    "    # List your alpha IDs\n",
    "    alpha_ids = [\"g2gngXg\"]                                      #Add Parent Alpha IDs\n",
    "\n",
    "    for alpha_id in alpha_ids:\n",
    "        print(f\"Processing Alpha ID: {alpha_id}\", flush=True)\n",
    "\n",
    "        # Step 1: Generate English description of the alpha\n",
    "        alpha_description = await generate_alpha_description(alpha_id, brain_session)\n",
    "        print(f\"\\nAlpha Description:\\n{alpha_description}\", flush=True)\n",
    "        await asyncio.sleep(0.1)\n",
    "\n",
    "        # Step 2: Generate new alphas based on the description\n",
    "        new_alphas = await generate_new_alphas(alpha_description, brain_session)\n",
    "        print(f\"\\nNew Alphas:\\n{new_alphas}\",flush=True)\n",
    "        await asyncio.sleep(0.1)\n",
    "\n",
    "        # Step 3: Create new alphas in the Brain\n",
    "        for alpha in new_alphas:\n",
    "            try:\n",
    "                simulate_data = ace.generate_alpha(regular=alpha['expression'])\n",
    "                simulation_result = ace.simulate_single_alpha(brain_session, simulate_data)\n",
    "\n",
    "                child_alpha_id = simulation_result.get('alpha_id')\n",
    "                if not child_alpha_id:\n",
    "                    record_feedback(\n",
    "                        problem=f\"Simulation failed for: {alpha['expression']}\",\n",
    "                        fix=\"Avoid similar structure or operators next time.\"\n",
    "                    )\n",
    "                    print(f\"⚠️ Failed to generate alpha for {alpha['alpha_name']}\", flush=True)\n",
    "                    continue\n",
    "\n",
    "                # Generate description for child alpha\n",
    "                base_desc = await generate_alpha_description(child_alpha_id, brain_session)\n",
    "\n",
    "                # Construct structured 100+ word description\n",
    "                idea_text = f\"Idea:\\n{base_desc}\\n\\n\"\n",
    "\n",
    "                data_rationale = (\n",
    "                    \"Rationale for data used:\\n\"\n",
    "                    \"The selected data fields capture complementary aspects of market behavior. \"\n",
    "                    \"By combining datasets such as short interest and fundamental indicators, \"\n",
    "                    \"the alpha draws insights from both price-based dynamics and firm-specific signals. \"\n",
    "                    \"This ensures robustness across varying market regimes and minimizes dependence \"\n",
    "                    \"on a single source of predictive power.\\n\\n\"\n",
    "                )\n",
    "\n",
    "                operator_rationale = (\n",
    "                    \"Rationale for operators used:\\n\"\n",
    "                    \"The operators employed—such as quantile, ts_backfill, and other time-series functions—\"\n",
    "                    \"enable normalization, smoothing, and statistical enhancement of noisy raw data. \"\n",
    "                    \"Quantile operations help establish relative rankings, while time-series operators \"\n",
    "                    \"capture evolving momentum or mean-reversion trends. Together, these choices help \"\n",
    "                    \"generate cleaner and more consistent alpha signals across time and assets.\"\n",
    "                )\n",
    "\n",
    "                child_description = (idea_text + data_rationale + operator_rationale).strip()\n",
    "\n",
    "                # Ensure the total description has at least 100 words\n",
    "                if len(child_description.split()) < 100:\n",
    "                    child_description += (\n",
    "                        \"\\n\\nOverall, this alpha integrates multiple sources of data and \"\n",
    "                        \"statistical operators to produce a diversified, low-correlation, \"\n",
    "                        \"and interpretable investment signal that aims for consistent performance \"\n",
    "                        \"under varying market conditions.\"\n",
    "                    )\n",
    "\n",
    "                # Set Alpha Properties\n",
    "                ace.set_alpha_properties(\n",
    "                    brain_session,\n",
    "                    child_alpha_id,\n",
    "                    tags=[f\"{alpha_id}\"],\n",
    "                    regular_desc=child_description\n",
    "                )\n",
    "\n",
    "                print(f\"✅ Created new Alpha ID: {child_alpha_id} (parent: {alpha_id}) with name: {alpha['alpha_name']}\", flush=True)\n",
    "                await asyncio.sleep(0.1)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error while processing child alpha for {alpha_id}: {e}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f95d25d0-8664-4fc4-903f-5533d5295103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T14:29:06.863652Z",
     "iopub.status.busy": "2025-10-08T14:29:06.863217Z",
     "iopub.status.idle": "2025-10-08T14:31:37.074300Z",
     "shell.execute_reply": "2025-10-08T14:31:37.073575Z",
     "shell.execute_reply.started": "2025-10-08T14:29:06.863623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete biometrics authentication and press any key to continue: \n",
      "https://api.worldquantbrain.com/authentication/persona?inquiry=inq_M4RY5naQNdUNZzKpJJqAnEBucjAp\n",
      "\n",
      "Processing Alpha ID: g2gngXg\n",
      "\n",
      "[Success] Alpha details and settings fetched.\n",
      "\n",
      "Parsing alpha expression to indentify components...\n",
      " > operators identified: ['rank', 'ts_mean']\n",
      " > data fields identified: ['anl10_epsinnovate_increase_fy1']\n",
      "\n",
      "Fetching descriptions for data fields...\n",
      " > searching for 'anl10_epsinnovate_increase_fy1'...\n",
      "→ Sending prompt to Gemini 2.5 Flash Lite...\n",
      "\n",
      "Alpha Description:\n",
      "This alpha aims to identify stocks that have shown a **strong upward trend in the number of analysts making positive revisions to their earnings per share forecasts for the upcoming fiscal year (FY1)** over the past approximately 5 months (126 trading days).\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "*   **`anl10_epsinnovate_increase_fy1`**: This is the raw data we're looking at. It represents the \"Number of analysts with innovative positive revisions for earnings per share FY1.\" In simpler terms, it's a count of how many analysts are revising their predictions for a company's future earnings upwards in a positive way.\n",
      "*   **`ts_mean(..., 126)`**: This part calculates the **average** of the `anl10_epsinnovate_increase_fy1` value over the **last 126 trading days**. This smooths out daily fluctuations and gives us a sense of the recent trend. Think of it as a rolling average over about half a year.\n",
      "*   **`rank(...)`**: Finally, the `rank` operator takes this smoothed average (the 126-day mean of analyst revisions) and **ranks all the stocks** based on this value. It assigns a rank between 0.0 and 1.0, where higher values mean a higher rank.\n",
      "\n",
      "**In essence, this alpha is looking for stocks where the average number of positive earnings revisions by analysts has been increasing over the last 126 days, and it ranks them based on how strong this recent positive trend is.** Stocks with a higher rank are those that have seen the most significant and consistent upward momentum in positive analyst sentiment regarding their future earnings.\n",
      "⚠️ Truncating 81 records to 30 to reduce prompt size.\n",
      "⚠️ Truncating 1942 records to 50 to reduce prompt size.\n",
      "→ Sending prompt to Gemini 2.5 Flash Lite...\n",
      "✅ Successfully parsed new alphas as JSON.\n",
      "\n",
      "New Alphas:\n",
      "[{'alpha_name': 'Alpha_1', 'expression': 'rank(ts_decay_exp_window(divide(anl10_epsinnovate_increase_fy1, anl10_cpssmun_2qf_2376), 126, 0.7))'}, {'alpha_name': 'Alpha_2', 'expression': 'group_rank(subtract(ts_mean(anl14_actvalue_eps_fy0, 90), ts_mean(anl14_actvalue_eps_fy0, 210)), industry)'}, {'alpha_name': 'Alpha_3', 'expression': 'ts_zscore(signed_power(subtract(anl10_prrinnovation_score_fq1_2561, anl10_ebiinnovate_decrease_fq1_2580), 1.5), 60)'}, {'alpha_name': 'Alpha_4', 'expression': 'normalize(divide(ts_std_dev(anl14_grmfy1_consensus_847, 180), ts_mean(anl14_grmfy1_consensus_847, 180)))'}, {'alpha_name': 'Alpha_5', 'expression': 'rank(ts_covariance(anl10_ndtfy1_pred_surps_v0_2774, anl14_numofests_ntprep_fy1, 90))'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 15:50:46,715 - ace - ERROR - Simulation failed. {'id': '3BRpOp69w4Ny8HfEd8Hf67q', 'type': 'REGULAR', 'status': 'ERROR', 'message': 'Got invalid value \"0.7\" for attribute \"lookback\", should be a positive integer', 'location': {'line': 1, 'start': 5, 'end': 98, 'property': 'regular'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to generate alpha for Alpha_1\n",
      "\n",
      "[Success] Alpha details and settings fetched.\n",
      "\n",
      "Parsing alpha expression to indentify components...\n",
      " > operators identified: ['group_rank', 'subtract', 'ts_mean']\n",
      " > data fields identified: ['anl14_actvalue_eps_fy0', 'industry']\n",
      "\n",
      "Fetching descriptions for data fields...\n",
      " > searching for 'anl14_actvalue_eps_fy0'...\n",
      " > searching for 'industry'...\n",
      "→ Sending prompt to Gemini 2.5 Flash Lite...\n",
      "✅ Created new Alpha ID: leqealgn (parent: g2gngXg) with name: Alpha_2\n",
      "\n",
      "[Success] Alpha details and settings fetched.\n",
      "\n",
      "Parsing alpha expression to indentify components...\n",
      " > operators identified: ['signed_power', 'subtract', 'ts_zscore']\n",
      " > data fields identified: ['anl10_ebiinnovate_decrease_fq1_2580', 'anl10_prrinnovation_score_fq1_2561']\n",
      "\n",
      "Fetching descriptions for data fields...\n",
      " > searching for 'anl10_ebiinnovate_decrease_fq1_2580'...\n",
      " > searching for 'anl10_prrinnovation_score_fq1_2561'...\n",
      "→ Sending prompt to Gemini 2.5 Flash Lite...\n",
      "✅ Created new Alpha ID: 2r0r59wJ (parent: g2gngXg) with name: Alpha_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 15:54:37,990 - ace - ERROR - Simulation failed. {'id': '2aAoPw9WW4SBahINzD9YtAt', 'type': 'REGULAR', 'status': 'ERROR', 'message': 'Attempted to use unknown variable \"anl14_grmfy1_consensus_847\"', 'location': {'line': 1, 'start': 28, 'end': 54, 'property': 'regular'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Failed to generate alpha for Alpha_4\n",
      "\n",
      "[Success] Alpha details and settings fetched.\n",
      "\n",
      "Parsing alpha expression to indentify components...\n",
      " > operators identified: ['rank', 'ts_covariance']\n",
      " > data fields identified: ['anl10_ndtfy1_pred_surps_v0_2774', 'anl14_numofests_ntprep_fy1']\n",
      "\n",
      "Fetching descriptions for data fields...\n",
      " > searching for 'anl10_ndtfy1_pred_surps_v0_2774'...\n",
      " > searching for 'anl14_numofests_ntprep_fy1'...\n",
      "→ Sending prompt to Gemini 2.5 Flash Lite...\n",
      "✅ Created new Alpha ID: ZY5YXeL1 (parent: g2gngXg) with name: Alpha_5\n"
     ]
    }
   ],
   "source": [
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
