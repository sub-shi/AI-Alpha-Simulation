{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47d58b3d-f7fe-439e-993f-8c161cc854f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T14:29:03.778791Z",
     "iopub.status.busy": "2025-10-08T14:29:03.778475Z",
     "iopub.status.idle": "2025-10-08T14:29:06.859694Z",
     "shell.execute_reply": "2025-10-08T14:29:06.858971Z",
     "shell.execute_reply.started": "2025-10-08T14:29:03.778768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete biometrics authentication and press any key to continue: \n",
      "https://api.worldquantbrain.com/authentication/persona?inquiry=inq_mVj4rK1bYJEctqsPGh45H5YBETFa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from click import prompt\n",
    "import pandas as pd\n",
    "import ace_lib as ace\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from google import genai  # Using Google AI studio\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "FEEDBACK_FILE = \"feedback_memory.json\"\n",
    "\n",
    "def load_feedback_memory():\n",
    "    if os.path.exists(FEEDBACK_FILE):\n",
    "        with open(FEEDBACK_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {\"mistakes\": [], \"corrections\": []}\n",
    "\n",
    "def save_feedback_memory(memory):\n",
    "    with open(FEEDBACK_FILE, \"w\") as f:\n",
    "        json.dump(memory, f, indent=4)\n",
    "\n",
    "def record_feedback(problem, fix):\n",
    "    memory = load_feedback_memory()\n",
    "    memory[\"mistakes\"].append(problem)\n",
    "    memory[\"corrections\"].append(fix)\n",
    "    save_feedback_memory(memory)\n",
    "\n",
    "def build_feedback_context():\n",
    "    memory = load_feedback_memory()\n",
    "    if not memory[\"mistakes\"]:\n",
    "        return \"\"\n",
    "    summary = \"\\n\".join([\n",
    "        f\"- Previously, you made this mistake: '{m}' → Corrected as: '{c}'\"\n",
    "        for m, c in zip(memory[\"mistakes\"], memory[\"corrections\"])\n",
    "    ])\n",
    "    return f\"\\nImportant: Avoid repeating previous mistakes.\\n{summary}\\n\"\n",
    "\n",
    "\n",
    "# Small helper to handle rate limits / transient errors when calling Brain API\n",
    "def get_with_retries(session, url, params=None, max_retries=5, backoff_base=1.0):\n",
    "    \"\"\"Perform session.get with retries on 429 and 5xx errors.\n",
    "\n",
    "    Args:\n",
    "        session: requests-like session with .get\n",
    "        url: URL to fetch\n",
    "        params: optional params dict\n",
    "        max_retries: number of attempts\n",
    "        backoff_base: base seconds for exponential backoff\n",
    "\n",
    "    Returns:\n",
    "        Response object\n",
    "\n",
    "    Raises:\n",
    "        requests.exceptions.RequestException or the last exception encountered\n",
    "    \"\"\"\n",
    "    last_exc = None\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if params is not None:\n",
    "                r = session.get(url, params=params)\n",
    "            else:\n",
    "                r = session.get(url)\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            wait = backoff_base * (2 ** attempt) + random.random()\n",
    "            print(f\"Request exception: {e}. Retrying in {wait:.1f}s (attempt {attempt+1}/{max_retries})\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "\n",
    "        # If rate limited or server error, retry with backoff\n",
    "        if r.status_code == 429 or (500 <= r.status_code < 600):\n",
    "            wait = backoff_base * (2 ** attempt) + random.random()\n",
    "            print(f\"HTTP {r.status_code} received from {url}. Retrying in {wait:.1f}s (attempt {attempt+1}/{max_retries})\")\n",
    "            time.sleep(wait)\n",
    "            last_exc = Exception(f\"HTTP {r.status_code}\")\n",
    "            continue\n",
    "\n",
    "        # Success\n",
    "        return r\n",
    "\n",
    "    # All retries failed\n",
    "    if last_exc:\n",
    "        raise last_exc\n",
    "    raise Exception(\"Failed to get response\")\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "_llm_instance = None\n",
    "\n",
    "\n",
    "# Load API key from .env and initialize Gemini client (new SDK)\n",
    "load_dotenv()\n",
    "GENAI_API_KEY = os.getenv(\"GENAI_API_KEY\")\n",
    "if not GENAI_API_KEY:\n",
    "    print(\"⚠️ Warning: GENAI_API_KEY not set in environment. Please set it in .env or the environment.\")\n",
    "GEMINI_MODEL = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-flash-lite\")\n",
    "client = genai.Client(api_key=GENAI_API_KEY)\n",
    "\n",
    "async def call_llm(prompt):\n",
    "    \"\"\"\n",
    "    Async interface with Google Gemini API using google-genai SDK.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"→ Sending prompt to Gemini 2.5 Flash Lite...\")\n",
    "\n",
    "        # Run the sync model call in a separate thread to stay async-safe\n",
    "        response = await asyncio.to_thread(\n",
    "            client.models.generate_content,\n",
    "            model=GEMINI_MODEL,\n",
    "            contents=(f\"You are a Quantitative Finance expert. While avoiding same combination of operators, help users explain alphas clearly. {prompt}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Extract text response\n",
    "        if hasattr(response, \"text\"):\n",
    "            return response.text.strip()\n",
    "        elif hasattr(response, \"candidates\"):\n",
    "            return response.candidates[0].content.parts[0].text.strip()\n",
    "        else:\n",
    "            print(\"⚠️ No valid response content.\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Gemini: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate English Description for Alpha\n",
    "async def generate_alpha_description(alpha_id, brain_session):\n",
    "    try:\n",
    "        #1. Fetch Alpha Details (use get_with_retries to handle rate limiting)\n",
    "        url = f\"https://api.worldquantbrain.com/alphas/{alpha_id}\"\n",
    "        response = get_with_retries(brain_session, url)\n",
    "        response.raise_for_status()\n",
    "        details = response.json()\n",
    "        alpha_expression = details.get('regular', {}).get('code', '')\n",
    "        alpha_settings = details.get('settings', {})\n",
    "        print(\"\\n[Success] Alpha details and settings fetched.\")\n",
    "        \n",
    "\n",
    "        # If needed get operators or other data\n",
    "        operators = ace.get_operators(brain_session)\n",
    "        operator_id_column = 'name'\n",
    "        all_operators_names = set(operators[operator_id_column])\n",
    "\n",
    "        #Parse alpha expression to find used operators\n",
    "        print(\"\\nParsing alpha expression to indentify components...\")\n",
    "        tokens = set(re.findall(r'[a-zA-Z0-9_.]+', alpha_expression))\n",
    "        operators_used = sorted(list(tokens.intersection(all_operators_names)))\n",
    "        potential_data_fields = tokens - all_operators_names\n",
    "        data_fields_used = sorted([f for f in potential_data_fields if not f[0].isdigit() and len(f) > 4])\n",
    "        print(f\" > operators identified: {operators_used}\")\n",
    "        print(f\" > data fields identified: {data_fields_used}\")\n",
    "\n",
    "        # 4. Fetch context for indentified components\n",
    "        #Operators context\n",
    "        operators_context = \"[]\"\n",
    "        if operators_used:\n",
    "            operators_df = operators[operators[operator_id_column].isin(operators_used)]\n",
    "            operators_context = operators_df[[operator_id_column, 'description', 'definition']].to_json(orient='records')\n",
    "\n",
    "        #Data fields context\n",
    "        data_field_context_list = []\n",
    "        if data_fields_used:\n",
    "            print(f\"\\nFetching descriptions for data fields...\")\n",
    "            for field in data_fields_used:\n",
    "                print(f\" > searching for '{field}'...\")\n",
    "                # Use alpha's specific settings for an accurate search\n",
    "                field_df = ace.get_datafields(\n",
    "                    s=brain_session,\n",
    "                     search=field,\n",
    "                      region=alpha_settings.get('region', 'USA'),\n",
    "                      universe=alpha_settings.get('universe', 'TOP3000'),\n",
    "                       delay=alpha_settings.get('delay', 1),\n",
    "                       data_type= 'ALL' #Search across all data types\n",
    "                )\n",
    "                if not field_df.empty:\n",
    "                    #Find the exact match\n",
    "                    exact_match = field_df[field_df['id'] == field]\n",
    "                    if not exact_match.empty:\n",
    "                        data_field_context_list.append(exact_match)\n",
    "                    else:\n",
    "                        print(f\"    -Warning: Found potential match for '{field}', but no exact match.\")\n",
    "                else:\n",
    "                    print(f\"    -Warning: No datafield found for '{field}' with current alpha settings.\")\n",
    "        \n",
    "        data_field_context = \"[]\"\n",
    "        if data_field_context_list:\n",
    "            final_df = pd.concat(data_field_context_list, ignore_index=True)\n",
    "            data_field_context = final_df[['id', 'description']].to_json(orient='records')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating alpha description: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Generate English description using call_llm\n",
    "    \n",
    "    operators_json = operators[operators['scope'] == 'REGULAR'][['name', 'description', 'definition']].to_json(orient='records')\n",
    "    data_json = data_field_context if isinstance(data_field_context, str) else json.dumps(data_field_context)\n",
    "\n",
    "    # Now build the prompt\n",
    "    prompt = f\"\"\"\n",
    "    Describe the following alpha in plain English.\n",
    "    Alpha: {alpha_expression}\n",
    "    Here are the components in JSON:\n",
    "    \"operators\": {operators_json},\n",
    "    \"data_fields\": {data_json}\n",
    "    \"\"\"\n",
    "    description = await call_llm(prompt)\n",
    "    if description:\n",
    "        return description.strip()\n",
    "    else:\n",
    "        print(\"⚠️ LLM returned no response.\")\n",
    "        return \"Error: LLM returned no response.\"\n",
    "\n",
    "\n",
    "# Generate new Alphas based on generated description\n",
    "async def generate_new_alphas(alpha_description, brain_session): \n",
    "    num_alphas = 5                                   #Number of Alphas\n",
    "\n",
    "    # Fetch operator and data context\n",
    "    operators = ace.get_operators(brain_session)\n",
    "    dataset_ids = ['analyst10', 'analyst14']             # Choosing datasets\n",
    "    data_fields = pd.concat(\n",
    "        [\n",
    "            ace.get_datafields(\n",
    "                brain_session,\n",
    "                region='USA',\n",
    "                universe='TOP3000',\n",
    "                delay=1,\n",
    "                dataset_id=dataset_id,\n",
    "                data_type='ALL'\n",
    "            )\n",
    "            for dataset_id in dataset_ids\n",
    "        ],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Convert to JSON\n",
    "\n",
    "    def truncate_json_records(df, cols, max_records):\n",
    "        if len(df) > max_records:\n",
    "            print(f\"⚠️ Truncating {len(df)} records to {max_records} to reduce prompt size.\")\n",
    "            df = df.sample(max_records, random_state=42)\n",
    "        return df[cols].to_json(orient='records')\n",
    "\n",
    "    operators_json = truncate_json_records(\n",
    "        operators[operators['scope'] == 'REGULAR'], \n",
    "        ['name', 'description', 'definition'], \n",
    "        30\n",
    "    )\n",
    "\n",
    "    data_fields_json = truncate_json_records(\n",
    "        data_fields, \n",
    "        ['id', 'description', 'category', 'type'], \n",
    "        50\n",
    "    )\n",
    "\n",
    "    feedback_context = build_feedback_context()\n",
    "\n",
    "    # Build the prompt — concise and strict\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following description: '{alpha_description}', generate {num_alphas} new alpha expressions using the provided operators and data.\n",
    "    {feedback_context}\n",
    "    Operators: {operators[operators['scope']=='REGULAR'].to_json()}, data {data_fields.to_json()} where id is data field name\n",
    "    Important: You can use type=MATRIX field by itself, as input to Arithmetic, \n",
    "    Cross Sectional, Time Series operators, With Logical and Transformational operators, As group in Group operators, with bucket().\n",
    "    You can’t use type=VECTOR field by itself. You only can use type=VECTOR field as input to Vector operator. Then you can treat it as a MATRIX field.\n",
    "    Always wrap type=VECTOR data in category=Vector operator.\n",
    "    You can’t use type=GROUP field by itself. You need to use it as “group” parameter in Group operator.\n",
    "\n",
    "    Provide only {num_alphas} alpha expressions, they should not be the same.\n",
    "    \"\"\"\n",
    "\n",
    "    # Call the LLM\n",
    "    response = await call_llm(prompt)\n",
    "    if not response:\n",
    "        print(\"⚠️ LLM returned no response while generating new alphas.\")\n",
    "        return []\n",
    "\n",
    "    # Try to parse as JSON safely\n",
    "    new_alphas_json = []  # Ensure variable exists even if parsing fails\n",
    "    try:\n",
    "        clean_response = response.strip()\n",
    "        # Remove Markdown fences like ```json or ```\n",
    "        clean_response = re.sub(r\"^```(?:json)?|```$\", \"\", clean_response, flags=re.MULTILINE).strip()\n",
    "\n",
    "        # Parse JSON\n",
    "        new_alphas_json = json.loads(clean_response)\n",
    "        print(\"✅ Successfully parsed new alphas as JSON.\")\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"⚠️ Response is not valid JSON. Attempting fallback parsing.\")\n",
    "        clean_lines = [\n",
    "            line.strip()\n",
    "            for line in response.split(\"\\n\")\n",
    "            if line.strip() and not line.strip().startswith(\"```\")\n",
    "        ]\n",
    "        new_alphas_json = [\n",
    "            {\"alpha_name\": f\"Alpha_{i+1}\", \"expression\": line}\n",
    "            for i, line in enumerate(clean_lines)\n",
    "        ]\n",
    "\n",
    "    # Remove duplicates by operator signature and record feedback\n",
    "    if new_alphas_json:\n",
    "        def operator_signature(expr):\n",
    "            ops = re.findall(r\"[a-z_]+\", expr)\n",
    "            return \"_\".join(sorted(set(ops)))\n",
    "\n",
    "        unique_alphas = []\n",
    "        seen_sigs = set()\n",
    "\n",
    "        for alpha in new_alphas_json:\n",
    "            sig = operator_signature(alpha[\"expression\"])\n",
    "            if sig in seen_sigs:\n",
    "                record_feedback(\n",
    "                    problem=f\"Repeated operator combination: {sig}\",\n",
    "                    fix=\"Enforce stronger variation across economic themes and operator types.\"\n",
    "                )\n",
    "                continue\n",
    "            seen_sigs.add(sig)\n",
    "            unique_alphas.append(alpha)\n",
    "\n",
    "        new_alphas_json = unique_alphas\n",
    "    else:\n",
    "        print(\"⚠️ No valid alphas generated after parsing.\")\n",
    "\n",
    "    return new_alphas_json\n",
    "\n",
    "# Start Brain session\n",
    "brain_session = ace.start_session()\n",
    "\n",
    "async def main():\n",
    "    #alpha_description = \"put Alpha Description here after generation\"\n",
    "\n",
    "\n",
    "    # List your alpha IDs\n",
    "    alpha_ids = [\"g2gngXg\"]                                      #Add Parent Alpha IDs\n",
    "\n",
    "    for alpha_id in alpha_ids:\n",
    "        print(f\"Processing Alpha ID: {alpha_id}\", flush=True)\n",
    "\n",
    "        # Step 1: Generate English description of the alpha\n",
    "        alpha_description = await generate_alpha_description(alpha_id, brain_session)\n",
    "        print(f\"\\nAlpha Description:\\n{alpha_description}\", flush=True)\n",
    "        await asyncio.sleep(0.1)\n",
    "\n",
    "        # Step 2: Generate new alphas based on the description\n",
    "        new_alphas = await generate_new_alphas(alpha_description, brain_session)\n",
    "        \n",
    "        print(f\"\\nNew Alphas:\\n{new_alphas}\",flush=True)\n",
    "        await asyncio.sleep(0.1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a64fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c7dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_list = [\"Alpha Expressions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dde0d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [\n",
    "    ace.generate_alpha(\n",
    "        x,\n",
    "        region=\"USA\",\n",
    "        universe=\"TOP3000\",\n",
    "        test_period = \"P2Y\",\n",
    "    )\n",
    "    for x in expression_list\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752b1f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ace.simulate_alpha_list_multi(brain_session, alpha_list[:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
